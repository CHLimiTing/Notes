#  鲁棒连续聚类——Robust Continuous Clustering

[2017_PNAS_[University of Maryland]_Robust continuous clustering_Sohil Atul Shah.pdf](2017_PNAS_[University of Maryland]_Robust continuous clustering_Sohil Atul Shah.pdf) 



## 1 引言

本文提出了一种基于鲁棒统计学的连续聚类算法（RCC），旨在解决高维数据和大数据集中传统聚类算法效果有限的问题。

RCC算法通过**优化一个平滑的连续目标函数**，实现了高效、准确的聚类，且无需预先指定聚类数目。算法基于再下降的鲁棒估计器，能够解开高度混合的聚类，并通过标准线性最小二乘求解器进行优化，确保了高效性和可扩展性。

此外，RCC算法还能与特征学习管道集成，实现联合聚类和降维（RCC-DR），进一步提高了聚类的效果。



## 2 RCC 算法概述

公式是基于最近的聚类凸松弛。然而，在求解过程中，将目标函数进行局部近似凸，在算法迭代过程中逐步加入非凸，即目标故意不是凸的。我们使用降阶的鲁棒估计器，通过优化单个连续目标，即使是高度混合的集群也可以被解开。尽管目标具有非凸性，但仍然可以使用标准线性最小二乘求解器来执行优化，这是高效且可扩展的。由于该算法将聚类表达为基于鲁棒估计的连续目标的优化，因此称其为鲁棒连续聚类(robust continuous clustering，RCC)。

RCC的目标函数结合了数据项和成对项（pairwise terms），用于描述数据点与其代表（representatives）之间的距离，以及代表之间的相互作用。

> 我们考虑一组 n 个数据点的聚类问题。输入 **X = [ x~1~，x~2~，… , x~n~ ]**，其中 x~i~ ∈ R^D^ 。我们方法作用于一组代表点 **U =  [u~1~，u~2~，...，u~n~]**，其中 u~i~ ∈ R^D^ 。代表点集 **U** 在对应的数据点 **X** 处初始化优化对代表点 **U** 进行操作，**U** 合并以揭示数据中潜在的聚类结构。因此，聚类的数量不需要事先知道。**U** 的优化过程如下图所示。
>
> ![image-20240911165015765](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409151928876.png)



RCC的目标有以下形式：

![image-20240911164921672](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409111649774.png)

其中 ***ε*** 是连接数据点的边集，函数 *ρ*(·)是对正则化项的惩罚，图通过互k-最近邻( m-kNN )方法构建的，它比常用的kNN图更具鲁棒性。权重 w~p,q~ 平衡了每个数据点对两两项的贡献，λ 平衡了不同客观项的强度。



对于当前目标函数 [**1**] 有两个问题：

1. 由于我们希望来自同一潜在聚类的观测值的代表 **u~i~** 坍缩成一个单点，因此自然的惩罚是 *L*~0~ 范数 ( ρ(y) = [y ≠ 0]，其中[·]是**艾弗森括号**)。然而，这将目标转化为一个难以处理的**组合优化问题**。

   > L~0~ 范数：指向量中非零元素的个数。L~0~范数常常用来表示模型的复杂性或者模型的稀疏性
   >
   > L~1~ 范数：指向量中各个元素绝对值之和。L~1~范数因为其产生稀疏解的特性，是一种常用的特征选择方法，可以用来剔除不重要的特征。
   >
   > L~2~ 范数：指向量各元素的平方和然后求平方根。在线性回归模型中，L~2~范数可以用来进行正则化，防止模型过拟合。
   >
   > 
   >
   > 艾佛森括号 [·] ：如果方括号内的条件满足则为1，不满足则为0。
   >
   > ![image-20240911174659080](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409111746116.png)
   >
   > 
   >
   > 组合优化问题 (Combinatorial optimization problem，COP) 是一类在离散状态下求极值的最优化问题，其数学模型如下所示：
   >
   > ![image-20240912133036598](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409121330623.png)
   >
   > 其中 x 为决策变量、 F(x) 为目标函数、 G(x) 为约束条件， D 表示离散的决策空间，为有限个点组成的集合。

2. 凸函数对连通性结构 ***ε*** 中的虚假边的鲁棒性有限，因为在优化过程中，当代表分开时，虚假成对项的影响并不会减少。给定嘈杂的现实世界数据，跨不同底层聚类的连接对连接结构的严重污染是不可避免的。我们的方法使用鲁棒估计器来自动修剪虚假的簇间连接，同时保持真实的簇内对应，所有这些都在单个连续目标内。



目标 1 中的第二项与平均移位目标有关。RCC目标的不同之处在于它包括一个额外的数据项，使用稀疏(而不是完全连接)连接结构，并且基于鲁棒估计。

> 平均移位目标：MeanShift算法，又称为均值漂移算法，采用基于颜色特征的核密度估计，寻找局部最优，使得跟踪过程中对目标旋转，小范围遮挡不敏感。
> MeanShift的本质是一个迭代的过程，在一组数据的密度分布中，使用无参密度估计寻找到局部极值（不需要事先知道样本数据的概率密度分布函数，完全依靠对样本点的计算）。



我们的方法是基于鲁棒估计和线过程之间的对偶性。我们为每个连接 (p,q) ∈ ***ε*** 引入一个辅助变量 l~p,q~ ，并在代表 **U** 和线过程 L =  { l~p,q~ }上优化一个联合目标：

![image-20240912131540003](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409121315025.png)

这里 Ψ( l~p,q~ ) 是对忽略连接(p, q)的惩罚：Ψ( l~p,q~ ) 在连接处于活动状态时趋向于 0 ( l~p,q~ → 1 )，在连接被禁用时趋向于 1 ( l~p,q~ → 0 )。

即 p,q 之间有连接时，Ψ( l~p,q~ →1) → 0 ；即 p,q 之间无连接时，Ψ( l~p,q~ →0 ) → 1 。

各种各样的鲁棒估计器 ρ(·) 都有相应的惩罚 Ψ(·) ，使得目标 1 和 2 相对于 **U** 是等价的：优化这两个目标中的任何一个都会产生相同的代表 **U** 集合。这个公式与**迭代重加权最小二乘(IRLS)**有关，但由于显式变量 L 和在这些变量上定义附加项的能力，它更加灵活。

![image-20240921214430342](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409212144653.png)



> **迭代重加权最小二乘**（IRLS）是一种优化算法，它通过迭代的方式来改进传统最小二乘法对异常值的敏感性，用于解决具有鲁棒性的回归问题。基本思路是：
>
> 1.初始化权重：首先使用最小二乘法求解初始解，并给每个数据点分配初始权重（通常是全1）。
> 2.计算残差：根据当前的模型参数计算每个数据点的残差。
> 3.更新权重：根据残差的大小调整每个数据点的权重。通常，较大的残差会被赋予较小的权重，从而减少其对模型的影响。
> 4.重新拟合模型：使用更新后的权重重新进行最小二乘拟合，得到新的模型参数。
> 5.迭代：重复步骤2到4，直到模型参数稳定或达到预设的迭代次数。
>
> IRLS 通过迭代调整权重，使得算法更鲁棒于数据中的异常值或噪声，最终得到更为可靠的回归结果。



目标 2 可以通过任何基于梯度的方法进行优化。 然而，它的形式可以通过线性最小二乘系统的迭代解实现高效且可扩展的优化。

这产生了一种通用方法，可以容纳许多鲁棒的非凸函数 ρ，将聚类减少到高度优化的现成线性系统求解器的应用，并轻松扩展到数万维中包含数十万个点的数据集。 相比之下，考虑了一系列特定的凹惩罚，并推导出了一个计算密集型的优化-最小化方案，用于优化这种特殊情况下的目标。 我们的工作提供了一个高效的通用解决方案。 

虽然所提出的方法可以在相同的计算效率框架中容纳许多估计器，但我们的阐述和实验使用了众所周知的 Geman-McClure 估计器的一种形式：

![image-20240913162213230](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409131622303.png)

其中µ是尺度参数。使目标1和目标2相对于U等价的相应惩罚函数为：

![image-20240913162435443](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409131624479.png)



## 3 RCC算法的优化

为了找到数据点的代表 U（聚类中心）和边权重 L ，RCC算法通过迭代优化目标函数来计算，而这个目标函数被转化为**线性最小二乘**问题来求解。

> 线性最小二乘法：这个目标函数取得最小值时的函数参数，这就是最小二乘法的思想，所谓“二乘”就是平方的意思。从这里我们可以看到，**最小二乘法其实就是用来做函数拟合的一种思想**。
>
> ![image-20240913171118123](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409131711167.png)

整个优化流程包括以下几个关键步骤：

1. **目标函数转化**：首先，RCC算法将原始的非凸目标函数通过逐步非凸性（graduated nonconvexity）等技术转化为一个或多个可解的线性最小二乘问题。这个过程中，算法从局部凸近似开始，逐渐增加非凸性，以便最终找到全局最优解或接近全局最优的解。
2. **迭代优化**：在转化后的线性最小二乘问题上，RCC算法使用高效的线性求解器进行迭代优化。这包括更新数据点的代表（聚类中心）和边权重，以最小化目标函数。



目标 2 对于是 (U, L) 凸函数，当变量 U 固定时，各个成对项解耦，每个 l~p,q~ 的最优值可以以封闭形式独立计算。当变量 L 固定时，目标 2 就变成了一个线性最小二乘问题。我们利用这种特殊的结构，通过交替更新变量集 U 和 L 来优化目标。作为块坐标下降算法，这种交替最小化方案可证明是收敛的。

> **坐标下降法（Coordinate Descent）**：每次迭代只沿单一维度搜索，得到当前维度的极小值之后再循环沿其它维度搜索，最终收敛得到目标函数的极小值。也就是每次迭代只对一个变量进行优化，而固定其它变量的值。
>
> ​		对于目标函数 f(x) ，x={x~1~,x~2~,⋯,x~n~}，利用坐标下降法求解该函数的最小值过程可以表示为：
>
> ![image-20240913190801097](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409131908132.png)
>
> 块坐标下降法（Block Coordinate Descent，BCD）：块坐标下降法是坐标下降法的一般化，用于解决坐标下降法效率低下的问题。每次迭代**对变量的子集进行优化**，即每次沿着多个坐标轴的方向（超平面）取极值。其下降过程中子集的更新顺序可以是确定的也可以是随机的。



当 U 固定时，每个 l~p,q~ 的最优值由下式给出：

![image-20240913171347096](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409131713130.png)

这可以通过将公式 5 代入公式 2 来验证，得到关于 U 的目标 1 。

当 L 固定时，我们可以将 [2] 改写成矩阵形式，得到求解 U 的简化表达式：

![image-20240913171303712](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409131713756.png)

其中 e~i~ 是第 i 个元素设为 1 的指示向量。这是一个线性最小二乘问题，可以使用快速和可扩展的求解器有效地求解。线性最小二乘公式由下式给出:

![image-20240913174346359](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409131743402.png)

这里 I ∈ R^n×n^ 是单位矩阵。这很容易证明

![image-20240914143550297](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409141435370.png)

是一个拉普拉斯矩阵，因此M是对称的半正定的。与任何多网格求解器一样，Eq. 7中的每一行 U 都可以独立或并行求解。

注意，U 和 L 的所有更新都优化同一个连续全局目标 2 。

该算法使用了渐变非凸性。它从目标的局部凸近似开始，通过设置 µ 使估计量的二阶导数在域的相关部分上为正![image-20240914144015420](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409141440463.png)而获得。在迭代过程中，µ会自动减小，逐渐将非凸性引入目标。在某些假设下，已知这样的延拓方案可以获得接近全局最优的解。

RCC目标1中的参数λ平衡了数据项和成对项的强度。将RCC重新表述为线性最小二乘问题，可以自动设置 λ 。具体来说，Eq. 7表明数据项和成对项可以通过设置来平衡

![image-20240914144821328](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409141448365.png)

λ 的值在每次更新 µ 后都会根据这个公式自动更新。更新只涉及计算拉普拉斯矩阵 **A** 的最大特征值，**X **的谱范数在初始化时预先计算并重新使用。

![image-20240914234015014](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409142340074.png)

## 4 联合聚类与降维

RCC-DR（鲁棒连续聚类与降维）是RCC（鲁棒连续聚类）算法的扩展。RCC-DR通过优化一个连续的全局目标函数，同时进行聚类和降维。这种联合优化有助于通过关注对聚类最相关的特征来提高聚类性能。

RCC-DR算法通过同时学习数据的低维嵌入和聚类嵌入的数据点来实现聚类。这种联合优化有助于在降低维度的同时提高聚类的效果。

RCC-DR继承了RCC的吸引人的特性：聚类和降维是通过优化一个明确的连续目标来共同执行的，框架支持非凸鲁棒估计器，可以解结混合聚类，优化是通过高效和可扩展的数值方法来执行的。

> 什么是降维？
>
> 降维是指通过保留一些比较重要的特征，去除一些冗余的特征，减少数据特征的维度。而特征的重要性取决于该特征能够表达多少数据集的信息，也取决于使用什么方法进行降维。一般情况会先使用线性的降维方法再使用非线性的降维方法，通过结果去判断哪种方法比较合适。
>
> 在哪里用到降维？
>
> 1. 特征维度过大，可能会导致过拟合时
>
> 2. 某些样本数据不足的情况（缺失值很多）
>
> 3. 特征间的相关性比较大时
>
>
> 通过特征选择/降维的目的是：
>
> 1. 减少特征属性的个数，确保特征属性之间是相互独立的
> 2. 解决特征矩阵过大， 导致计算量比较大，训练时间长的问题，便于发现规律
>
> 端到端的概念表示 **模型可以直接利用输入数据而不需要其他处理** 。如果模型可以直接通过输入原始数据来得到输出，那么我们就说这个模型是端到端的（可以理解为从输入端直接到输出端的）。

首先考虑RCC-DR目标的初始公式：

![image-20240914160120581](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409141601622.png)

> ![image-20240915104643472](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409151046568.png)

这里 D ∈ R^D×d^ 是字典，z~i~ ∈ R^d^是对应于第 i 个数据样本的稀疏代码，u~i~ ∈ R^d^ 是 x~i~ 的低维嵌入。对于固定的 D，参数 ν 将稀疏编码目标中的数据项与约简空间中的聚类目标进行平衡。这个初始的公式 [10] 是有问题的，因为在优化开始时，由于尚未禁用的伪簇间连接，表示 U 可能会有噪声。这对原始RCC目标 [1] 的收敛性没有影响，但在公式 [10] 中，U 的污染可以通过 Z 感染稀疏编码系统并破坏字典 D 。出于这个原因，我们使用了一个不同的公式，它具有消除参数 ν 的额外好处：

![image-20240914160744821](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409141607851.png)

在这里，我们用鲁棒惩罚替换了约简空间中数据项上的 L2 惩罚。我们对 ρ1 和 ρ2 都使用了Geman-McClure估计器。

> ### RCC-DR算法步骤
>
> 1. **初始化**：初始化 u~i~ = x~i~，z~i~ = 0，并使用 **PCA 或 K-SVD** 初始化字典 D。
>
>    > 主成分分析（PCA）和奇异值分解（SVD）是两种强大的降维技术，广泛应用于数据科学、机器学习和信号处理等领域。
>    >
>    > PCA利用**投影**的思想（降维方法的一种），将多个变量转化为少数几个主成分，其中每个主成分都是原始变量的，也就是说，各主成分之间互不相关（正交）。从而这些主成分能够反映实例的绝大部分信息。
>    >
>    > PCA通过识别数据集中方差最大的方向，将高维数据投影到低维空间，保留数据的关键信息。SVD是一种更通用的技术，可以将矩阵分解为奇异值和奇异向量的乘积，用于降维、特征提取和数据分析。
>    >
>    > PCA和SVD在数学基础和应用场景上存在差异。PCA基于协方差矩阵的特征值分解，主要用于线性数据的降维。SVD基于矩阵的奇异值分解，适用于线性或非线性数据的降维和特征提取。
>
> 2. **构建连接图**：使用互相k近邻（m-kNN）构建图 ***ε*** 。
>
> 3. **优化**：交替更新 U，Z 和 D
>
>    - 使用近端梯度下降法更新 z~i~ 。
>    - 使用稀疏编码的闭式解更新 D。
>    - 通过求解线性最小二乘问题更新 u~i~ 。
>
> 4. **线过程变量**：更新鲁棒惩罚项的线过程变量 l~p,q~ 。
>
> 5. **收敛**：重复上述更新，直到收敛或达到最大迭代次数。

为了优化目标 [11] ，我们分别引入与约简空间中的数据和成对项对应的直线过程 L^1^ 和 L^2^，并在 U、Z、D、L^1^ 和 L^2^ 上优化一个联合目标。优化是通过对这些变量组进行块坐标下降来执行的。直线过程 L^1^ 和 L^2^ 可以按照 Eq. 5 的封闭形式更新。变量 U 通过求解线性系统来更新

![image-20240914162130023](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409141621063.png)

M 是由图结构推导的矩阵。其中，

![image-20240914162321576](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409141623612.png)

H为对角矩阵，*h*~i,i~ = *l*~i~^1^。字典 D 和代码 Z 使用主成分分析(PCA)进行初始化。[ K-SVD算法也可用于此目的 ] 变量 Z 通过加速的近端梯度下降步骤更新，

![image-20240914162708173](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409141627211.png)

其中![image-20240914163202240](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409141632264.png)和![image-20240914163245205](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409141632229.png)。这个![	](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409142007147.png)执行元素级软阈值：

![image-20240914200959028](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409142009064.png)

更新变量D，使用

![image-20240914201157737](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409142011772.png)

其中β是一个小的正则化值，设为![image-20240914201337121](https://gitee.com/daimahei/typora-image-bed/raw/master/images/202409142013154.png)。



> **RCC算法的优势**
>
> - **连续性和可集成性**：优化了一个平滑的连续目标函数，这使得它可以作为特征学习管道中的一个模块进行集成。
> - **鲁棒性**：鲁棒项使得其能够处理混合紧密的聚类，分离纠缠的簇，从而在所有数据集上实现高准确性。
> - **可扩展性**：利用高效的线性求解器，能够扩展到大规模高维数据集，克服了传统算法在大数据集上的局限性。
>
> 
>
> **RCC-DR的优势**
>
> - **可扩展性**：高效的优化方法使 RCC-DR 能够扩展到大规模高维数据集。
> - **鲁棒性**：使用鲁棒惩罚函数有助于处理噪声数据和混合聚类。
> - **集成降维**：同时学习低维嵌入和聚类，提高了聚类质量。

